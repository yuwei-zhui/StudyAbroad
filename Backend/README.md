# Backend Code For Task 8.1C 
For frontend sample android app: https://github.com/dgdeakin/Task8.1CAndroidAppExample

# Files
Three files are present, use the file that works for you (Feel free to edit).
1. main-directModel.py
2. main-pipeline.py
3. main-ollama.py (Need to download ollama, may skip steps 4, 5)

You can explore code from previous year from here:
https://github.com/sit3057082025/T-8.1C. 
Readme file is useful.

# Instructions:
- Almost similar to previous setups as in Task6.1D Backend code setup (For files 1 and 2).
- Detailed steps are in ReadME of this repo:
https://github.com/sit3057082025/BackendApiLLM_T6.1D
- For File 3, you need to install ollama.

## 1. First clone the repo or download to your local folder.
## 2. Run terminal in the project folder.
   - Make sure python is installed. https://www.python.org/downloads/
   - Create Virtual environment using this in terminal: ```python -m venv venv```
   - venv folder will be created.
   - now activate virtual environment
     - For MAC:
        ```source venv/bin/activate```
     - For Windows PS:
       ```.\venv\Scripts\Activate.ps1```
## 3. Now install the libraries
   - ```pip install Flask```
     (source: https://flask.palletsprojects.com/en/stable/installation/)
   - ```pip install accelerate```
   - ```pip install requests```
   - ```pip install transformers```
     (source: https://pypi.org/project/transformers/)
   - ```pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126```
     (source: https://pytorch.org/get-started/locally/)
## 4. Settings with HuggingFace (Skip this if using main-ollama.py and go to Step 6 below)
   - Signup for Huggingface Account here: https://huggingface.co/
   - Create access token from here: https://huggingface.co/settings/tokens
   - Get Model "google/gemma-3-1b-it" from https://huggingface.co/google/gemma-3-1b-it.
      Acknowledge licence.

   <img src="screenshots/acknowledge_licence.png" width="500" alt="Description">

   - Then run this in terminal:
     ```huggingface-cli login```

   - It will ask for hugging face token, provide and press enter. Then select Y.
     
## 5. To run the code in terminal (Skip this if using main-ollama.py and go to stop 6 below): 
```python main-directModel.py```

or 

```python main-pipeline.py```


# 6. Using python main-ollama.py
a) First download and install ollama from: https://ollama.com/download

b) In your new terminal, run:
   - ```ollama serve```
   - You should get:
   
   <img src="screenshots/img_ollama.png" width="700">
   
NOTE: You might need to pull model as: ```ollama pull llama3.2:latest```

c) Then, in your backend (this project) terminal (in new from above 6.b terminal), run
   ```python main-ollama.py```
   
d) Then, either test the route points in postman as below (Steps 7a, 7b).

e) Or run the android app.

f) On successful run you should get:

  - in Ollama serve terminal

<img src="screenshots/img.png" width="800">
   


   - in your backend terminal,

<img src="screenshots/img_1.png" width="800">


   
   - in you android app,

<img src="screenshots/img_2.png" width="200">


# 7. To retrieve check response:
## a) Using curl in terminal PS (as shown in image below):
- PS D:\> curl.exe -X POST http://localhost:5000/chat -d "userMessage=Where is Australia?"
  
<img src="screenshots/curl.png" alt="Android App Screenshot" width="700">

## b) using PostMan (as shown in image below)
METHOD: POST

URL: http://localhost:5000/chat

Body: x-www-form-urlencoded
- key: userMessage
- Value: Where is Australia?


<img src="screenshots/postman.png" alt="Android App Screenshot" width="400">


# c) In Sample Android app:
- Use JsonRequest or other tools:

<img src="screenshots/response_directmodel.png" alt="Android App Screenshot" width="200">

<img src="screenshots/response_pipeline.png" alt="Android App Screenshot" width="200">

# 8. Troubleshooting
1. port 5000 may not be available sometimes, use different post like 5001. Change in android app as well the port number.
2. Make sure to run  "huggingface-cli login" command in terminal and provide token.
3. Make sure to acknowledge licence in the model site such as https://huggingface.co/google/gemma-3-1b-it
4. Select most permissions while creating access token.
5. Try with different models based on quick access. https://huggingface.co/models
6. Use small models.

# 9. Good to know about:
1. Python https://www.python.org/
2. Virtual Environments https://docs.python.org/3/library/venv.html
3. Flask https://flask.palletsprojects.com/en/stable/
4. Huggingface Transformers https://huggingface.co/docs/transformers/en/index

Further references (From Previous)
[https://github.com/sit3057082025/T-8.1C](https://github.com/sit3057082025/T-8.1C)


# References:
1. Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.
https://github.com/huggingface/transformers
2. A simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, automatic mixed precision (including fp8), and easy-to-configure FSDP and DeepSpeed support
https://github.com/huggingface/accelerate
3. Ollama
https://ollama.com/
4. How to Build a Flask Python Web Application from Scratch
https://www.digitalocean.com/community/tutorials/how-to-make-a-web-application-using-flask-in-python-3
5. https://flask.palletsprojects.com/en/stable/
6. What is Postman?
https://www.postman.com/product/what-is-postman/
https://learning.postman.com/docs/getting-started/overview/
7. What is curl command? 
https://developer.ibm.com/articles/what-is-curl-command/
8. JSONObject
https://developer.android.com/reference/org/json/JSONObject
https://www.digitalocean.com/community/tutorials/android-jsonobject-json-parsing
9. Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1 and other large language models.
https://github.com/ollama/ollama
10. What are large language models?
https://www.cloudflare.com/en-gb/learning/ai/what-is-large-language-model/
https://www.ibm.com/think/topics/large-language-models
https://aws.amazon.com/what-is/large-language-model/
11. venv â€” Creation of virtual environments
https://docs.python.org/3/library/venv.html
12. What's the Difference Between Frontend and Backend in Application Development?
https://aws.amazon.com/compare/the-difference-between-frontend-and-backend/


# StudyAbroad AI Backend

è¿™æ˜¯StudyAbroadåº”ç”¨çš„AIåç«¯æœåŠ¡ï¼Œä½¿ç”¨Ollamaæä¾›æœ¬åœ°AIèŠå¤©åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ¤– åŸºäºOllamaçš„æœ¬åœ°AIèŠå¤©
- ğŸ“± æ”¯æŒAndroidåº”ç”¨çš„JSON API
- ğŸ‘¤ ç”¨æˆ·ä¸ªäººèµ„æ–™é›†æˆ
- ğŸ“ ç•™å­¦ç›¸å…³çš„æ™ºèƒ½å»ºè®®
- ğŸ”„ å‘åå…¼å®¹çš„è¡¨å•æ•°æ®æ”¯æŒ

## ç³»ç»Ÿè¦æ±‚

- Python 3.7+
- Ollama (ç”¨äºAIæ¨¡å‹)
- è‡³å°‘4GB RAM (æ¨è8GB+)

## å®‰è£…æ­¥éª¤

### 1. å®‰è£…Ollama

è®¿é—® [Ollamaå®˜ç½‘](https://ollama.ai) ä¸‹è½½å¹¶å®‰è£…Ollamaã€‚

### 2. ä¸‹è½½AIæ¨¡å‹

```bash
ollama pull llama3.2:latest
```

### 3. å¯åŠ¨OllamaæœåŠ¡å™¨

```bash
ollama serve
```

ä¿æŒè¿™ä¸ªç»ˆç«¯çª—å£æ‰“å¼€ã€‚

### 4. å®‰è£…Pythonä¾èµ–

```bash
cd Backend
pip install -r requirements.txt
```

### 5. å¯åŠ¨FlaskæœåŠ¡å™¨

#### æ–¹æ³•1: ä½¿ç”¨å¯åŠ¨è„šæœ¬ (æ¨è)
```bash
python start_server.py
```

#### æ–¹æ³•2: ç›´æ¥å¯åŠ¨
```bash
python main-ollama.py
```

## APIæ–‡æ¡£

### èŠå¤©ç«¯ç‚¹

**URL:** `POST /chat`

**è¯·æ±‚æ ¼å¼ (JSON):**
```json
{
    "message": "ç”¨æˆ·æ¶ˆæ¯",
    "userId": "ç”¨æˆ·ID",
    "academicProfile": "ç”¨æˆ·å­¦æœ¯èƒŒæ™¯ä¿¡æ¯"
}
```

**å“åº”æ ¼å¼:**
```json
{
    "response": "AIå›å¤",
    "status": "success"
}
```

**é”™è¯¯å“åº”:**
```json
{
    "error": "é”™è¯¯ä¿¡æ¯",
    "status": "error"
}
```

### å‘åå…¼å®¹

æœåŠ¡å™¨ä¹Ÿæ”¯æŒè¡¨å•æ•°æ®è¯·æ±‚ï¼š
```bash
curl -X POST http://localhost:5000/chat \
  -d "userMessage=Hello, can you help me with studying abroad?"
```

## Androidåº”ç”¨é…ç½®

### ç½‘ç»œé…ç½®

åœ¨Androidåº”ç”¨ä¸­ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æœåŠ¡å™¨åœ°å€ï¼š

- **Androidæ¨¡æ‹Ÿå™¨**: `http://10.0.2.2:5000/`
- **ç‰©ç†è®¾å¤‡**: `http://YOUR_COMPUTER_IP:5000/`

### ä¿®æ”¹IPåœ°å€

å¦‚æœä½¿ç”¨ç‰©ç†è®¾å¤‡æµ‹è¯•ï¼Œéœ€è¦ä¿®æ”¹ `ApiConfig.java` ä¸­çš„ `BASE_URL_DEVICE`ï¼š

```java
public static final String BASE_URL_DEVICE = "http://192.168.1.100:5000/"; // æ›¿æ¢ä¸ºä½ çš„IP
```

## æµ‹è¯•

### æµ‹è¯•æœåŠ¡å™¨

```bash
python test_server.py
```

### æ‰‹åŠ¨æµ‹è¯•

```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are the best universities for computer science?",
    "userId": "test_user",
    "academicProfile": "Major: Computer Science\nGPA: 3.8\nTarget: Master degree"
  }'
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿æ¥é”™è¯¯**
   - ç¡®ä¿OllamaæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ (`ollama serve`)
   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
   - éªŒè¯ç«¯å£5000æ²¡æœ‰è¢«å ç”¨

2. **æ¨¡å‹æœªæ‰¾åˆ°**
   ```bash
   ollama pull llama3.2:latest
   ```

3. **Androidè¿æ¥å¤±è´¥**
   - æ¨¡æ‹Ÿå™¨: ä½¿ç”¨ `10.0.2.2:5000`
   - ç‰©ç†è®¾å¤‡: ç¡®ä¿ç”µè„‘å’Œæ‰‹æœºåœ¨åŒä¸€ç½‘ç»œï¼Œä½¿ç”¨ç”µè„‘çš„IPåœ°å€

4. **æƒé™é”™è¯¯**
   - ç¡®ä¿Androidåº”ç”¨æœ‰ç½‘ç»œæƒé™
   - æ£€æŸ¥ `usesCleartextTraffic="true"` è®¾ç½®

### æ—¥å¿—æŸ¥çœ‹

æœåŠ¡å™¨ä¼šåœ¨æ§åˆ¶å°è¾“å‡ºè¯¦ç»†çš„è¯·æ±‚å’Œå“åº”æ—¥å¿—ï¼ŒåŒ…æ‹¬ï¼š
- æ¥æ”¶åˆ°çš„è¯·æ±‚
- Ollama APIè°ƒç”¨
- ç”Ÿæˆçš„å“åº”

## å¼€å‘è¯´æ˜

### é¡¹ç›®ç»“æ„

```
Backend/
â”œâ”€â”€ main-ollama.py      # ä¸»æœåŠ¡å™¨æ–‡ä»¶
â”œâ”€â”€ start_server.py     # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ test_server.py      # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ requirements.txt    # Pythonä¾èµ–
â””â”€â”€ README.md          # è¯´æ˜æ–‡æ¡£
```

### è‡ªå®šä¹‰é…ç½®

å¯ä»¥åœ¨ `main-ollama.py` ä¸­ä¿®æ”¹ï¼š
- AIæ¨¡å‹ (`MODEL` å˜é‡)
- ç”Ÿæˆå‚æ•° (temperature, top_pç­‰)
- ç«¯å£å· (é»˜è®¤5000)

### æ‰©å±•åŠŸèƒ½

è¦æ·»åŠ æ–°åŠŸèƒ½ï¼Œå¯ä»¥ï¼š
1. åœ¨Flaskåº”ç”¨ä¸­æ·»åŠ æ–°çš„è·¯ç”±
2. ä¿®æ”¹Androidåº”ç”¨çš„APIæ¥å£
3. æ›´æ–°æ•°æ®æ¨¡å‹

## è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚



